{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda0ca32",
   "metadata": {},
   "source": [
    "## Listing 8.1 Reweighting a probability distribution to a different temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d3a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6161450",
   "metadata": {},
   "source": [
    "# Listing 8.2 Downloading and parsing the initial text file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37ef536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "\n",
    "path=keras.utils.get_file('nietzsche.txt',\n",
    "origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a76c6a",
   "metadata": {},
   "source": [
    "## Listing 8.3 Vectorizing sequences of characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2834056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200281\n",
      "Unique characters: 59\n",
      "Vectorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_5048/1407214211.py:30: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_5048/1407214211.py:31: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "# You‚Äôll extract sequences \n",
    "# of 60 characters.\n",
    "maxlen=60\n",
    "# You‚Äôll sample a new sequence \n",
    "# every three characters.\n",
    "step=3\n",
    "#Holds the extracted sequences\n",
    "sentences = []\n",
    "#holds targets(the follow up charac)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0,len(text)-maxlen,step):\n",
    "    sentences.append(text[i:i+maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "    \n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "#List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary that maps\n",
    "# unique characters to their\n",
    "# index in the list ‚Äúchars‚Äù\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# One-hot encodes \n",
    "# the characters \n",
    "# into binary arrays\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfd1ca",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "This network is a single LSTM layer followed by a Dense classifier and softmax over all\n",
    " possible characters. But note that recurrent neural networks aren‚Äôt the only way to do\n",
    " sequence data generation; 1D convnets also have proven extremely successful at this\n",
    " task in recent times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b6c83",
   "metadata": {},
   "source": [
    "# Listing 8.4 Single-layer LSTM model for next-character prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6416b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6bd48aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizers\\legacy\\rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Because your targets are one-hot encoded, you‚Äôll use categorical_crossentropy as\n",
    "# the loss to train the model. \n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d061db5e",
   "metadata": {},
   "source": [
    " 1. Downloading and Preparing the Text Corpus\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n",
    ")\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))\n",
    "\n",
    "‚úÖ Explanation:\n",
    "\n",
    "    keras.utils.get_file(...): Downloads the Nietzsche text corpus from a URL and stores it locally.\n",
    "\n",
    "    open(path).read().lower():\n",
    "\n",
    "        Opens the downloaded file.\n",
    "\n",
    "        Reads the entire content as one long string.\n",
    "\n",
    "        Converts all characters to lowercase for normalization (i.e., to treat A and a as the same).\n",
    "\n",
    "    print('Corpus length:', len(text)): Shows total number of characters in the corpus.\n",
    "\n",
    "üìò 2. Setting Parameters for Sequence Extraction\n",
    "\n",
    "maxlen = 60  # length of each sequence to extract\n",
    "step = 3     # step size to slide the window\n",
    "\n",
    "‚úÖ Explanation:\n",
    "\n",
    "    maxlen = 60: Each input sequence will contain 60 characters.\n",
    "\n",
    "    step = 3: We‚Äôll extract a new sequence every 3 characters instead of 1 (to reduce data size and redundancy).\n",
    "\n",
    "üìò 3. Preparing Containers for Sequences and Next Characters\n",
    "\n",
    "sentences = []    # list to store 60-character sequences\n",
    "next_chars = []   # list to store the character that comes after each sequence\n",
    "\n",
    "üìò 4. Extracting Sequences and Targets Using Loop\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "‚úÖ Step-by-step breakdown:\n",
    "üîÅ for i in range(0, len(text) - maxlen, step):\n",
    "\n",
    "    Iterates from the start of the corpus up to (length - maxlen) to avoid out-of-bounds slicing.\n",
    "\n",
    "    Moves the pointer i by step (3 characters) in each iteration.\n",
    "\n",
    "üß† What happens inside the loop:\n",
    "\n",
    "    text[i: i + maxlen] extracts a 60-character sequence starting at index i.\n",
    "\n",
    "        Example: if i = 0, it gets characters 0‚Äì59.\n",
    "\n",
    "    This sequence is added to sentences with sentences.append(...).\n",
    "\n",
    "    text[i + maxlen] grabs the next character (the 61st character).\n",
    "\n",
    "        This is the target character the model will learn to predict.\n",
    "\n",
    "    This target character is added to next_chars.\n",
    "\n",
    "üîç Sample output (assume):\n",
    "\n",
    "If text = \"the quick brown fox jumps\" and maxlen=10, step=3, then:\n",
    "i\tSequence Extracted\tNext Char\n",
    "0\t\"the quick \"\t\"b\"\n",
    "3\t\"quick brow\"\t\"n\"\n",
    "6\t\"ck brown f\"\t\"o\"\n",
    "üìò 5. Getting Unique Characters and Creating Index Map\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "‚úÖ Explanation:\n",
    "\n",
    "    set(text): Finds all unique characters in the corpus.\n",
    "\n",
    "    sorted(...): Sorts the characters for consistency.\n",
    "\n",
    "    chars: List of all unique characters in order.\n",
    "\n",
    "    char_indices: Dictionary mapping each character to a unique index (used later for one-hot encoding).\n",
    "\n",
    "üìò 6. Vectorizing Sequences into One-Hot Encoded Format\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "‚úÖ Explanation:\n",
    "\n",
    "    x: A 3D NumPy array of shape (number of sequences, 60, number of unique characters).\n",
    "\n",
    "        For each sequence, for each character position, it's a one-hot vector of length = number of unique characters.\n",
    "\n",
    "    y: A 2D NumPy array of shape (number of sequences, number of unique characters).\n",
    "\n",
    "        For each target character (next char), a one-hot vector.\n",
    "\n",
    "üìò 7. Filling x and y with One-Hot Encoded Data\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "‚úÖ Step-by-step breakdown:\n",
    "üîÅ Outer loop: for i, sentence in enumerate(sentences)\n",
    "\n",
    "    Loops through each sentence (i is index, sentence is 60-char string).\n",
    "\n",
    "    Each sentence becomes one sample for input.\n",
    "\n",
    "üîÅ Inner loop: for t, char in enumerate(sentence)\n",
    "\n",
    "    Loops over each character (char) in the sequence.\n",
    "\n",
    "    t is the position of the character (0 to 59).\n",
    "\n",
    "    char_indices[char] gives the index for this character in the one-hot vector.\n",
    "\n",
    "    x[i, t, char_indices[char]] = 1 sets the appropriate one-hot vector to 1.\n",
    "\n",
    "üéØ Output vector:\n",
    "\n",
    "    y[i, char_indices[next_chars[i]]] = 1 sets the one-hot for the target character.\n",
    "\n",
    "‚úÖ Final Thoughts:\n",
    "\n",
    "This preprocessing pipeline prepares your text dataset for training a character-level language model using something like an LSTM in Keras. The model will learn to predict the next character given the previous 60 characters.\n",
    "\n",
    "Would you like a diagram or visual representation of the sequence slicing and vectorization process?\n",
    "You said:\n",
    "step 4 explain little bit more with the example and other steps also...\n",
    "ChatGPT said:\n",
    "\n",
    "Let‚Äôs walk through Step 4 in a bit more detail, complete with a concrete mini-example, and then revisit the other steps with that same example to make everything crystal-clear.\n",
    "Mini‚ÄêExample Corpus\n",
    "\n",
    "text = \"hello world!\"\n",
    "\n",
    "Here, len(text) = 12 characters (indices 0 through 11).\n",
    "\n",
    "Let‚Äôs choose:\n",
    "\n",
    "    maxlen = 5 (each sequence is 5 characters long)\n",
    "\n",
    "    step = 2 (we slide the window by 2 characters each time)\n",
    "\n",
    "Step 4: Extracting Sequences and Next Characters\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "    Loop bounds\n",
    "\n",
    "        range(0, len(text) - maxlen, step) ‚Üí range(0, 12 - 5, 2) ‚Üí indices i = 0, 2, 4, 6.\n",
    "\n",
    "    Iteration by iteration:\n",
    "\n",
    "    Iteration\ti\ttext[i : i+5]\ti+5\tNext char text[i+5]\tAppended to sentences / next_chars\n",
    "    1\t0\t\"hello\"\t5\t\" \" (space)\tsentences += [\"hello\"]\n",
    "    \t\t\t\t\tnext_chars += [\" \"]\n",
    "    2\t2\t\"llo w\"\t7\t\"o\"\tsentences += [\"llo w\"]\n",
    "    \t\t\t\t\tnext_chars += [\"o\"]\n",
    "    3\t4\t\"o wor\"\t9\t\"l\"\tsentences += [\"o wor\"]\n",
    "    \t\t\t\t\tnext_chars += [\"l\"]\n",
    "    4\t6\t\"world\"\t11\t\"!\"\tsentences += [\"world\"]\n",
    "    \t\t\t\t\tnext_chars += [\"!\"]\n",
    "\n",
    "    Resulting lists:\n",
    "\n",
    "sentences   # [\"hello\", \"llo w\", \"o wor\", \"world\"]\n",
    "next_chars  # [\" \",     \"o\",     \"l\",     \"!\"]\n",
    "\n",
    "    Each entry in sentences is exactly 5 characters.\n",
    "\n",
    "    Each corresponding entry in next_chars is the single character that immediately follows that 5-char block in the original text.\n",
    "\n",
    "Revisiting Other Steps with Our Mini-Example\n",
    "1. Unique Characters and Index Mapping\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "# e.g. chars = [' ', '!', 'd', 'e', 'h', 'l', 'o', 'r', 'w']\n",
    "\n",
    "char_indices = { char: idx for idx, char in enumerate(chars) }\n",
    "# e.g. { ' ':0, '!':1, 'd':2, 'e':3, ... }\n",
    "\n",
    "    We collect all distinct characters from \"hello world!\" and assign each one an index.\n",
    "\n",
    "2. Initializing One-Hot Arrays\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "\n",
    "    x.shape == (4, 5, 9) ‚Äî 4 sequences, 5 positions each, 9 unique chars.\n",
    "\n",
    "    y.shape == (4, 9) ‚Äî 4 targets, one-hot over 9 chars.\n",
    "\n",
    "3. Filling the One-Hot Arrays\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        idx = char_indices[char]\n",
    "        x[i, t, idx] = 1\n",
    "\n",
    "    # target character\n",
    "    target_idx = char_indices[next_chars[i]]\n",
    "    y[i, target_idx] = 1\n",
    "\n",
    "    Outer loop over the 4 sequences.\n",
    "\n",
    "    Inner loop over each character in a sequence (positions 0‚Äì4).\n",
    "\n",
    "        E.g. for i=0, sentence=\"hello\":\n",
    "\n",
    "            t=0, char='h' ‚Üí set x[0,0,char_indices['h']] = 1\n",
    "\n",
    "            t=1, char='e' ‚Üí set x[0,1,char_indices['e']] = 1\n",
    "\n",
    "            ‚Ä¶ and so on through 'o'.\n",
    "\n",
    "    After encoding all 5 chars, we one-hot encode the next char:\n",
    "\n",
    "        next_chars[0] is space \" \", so y[0,char_indices[\" \"]] = 1.\n",
    "\n",
    "4. What the Model Sees\n",
    "\n",
    "Each training example looks like:\n",
    "\n",
    "    Input: a 5√ó9 binary matrix (one-hot encoding of your 5-character block)\n",
    "\n",
    "    Target: a 9-dimensional one-hot vector (the next character)\n",
    "\n",
    "The model learns to map from the sequence of 5 characters to the probability distribution over the next character.\n",
    "Summary\n",
    "\n",
    "    Loop sliding by step lets you generate overlapping sequences more efficiently.\n",
    "\n",
    "    Slicing text[i : i+maxlen] always yields exactly maxlen characters.\n",
    "\n",
    "    Next character is what the model tries to predict (the label).\n",
    "\n",
    "    One-hot encoding transforms textual data into numeric arrays suitable for neural nets.\n",
    "\n",
    "Feel free to ask if you want more detail on any particular part!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18412d",
   "metadata": {},
   "source": [
    "### Listing 8.4 Single-layer LSTM model for next-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609fc5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.LSTM(128,input_shape=(maxlen,len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8b10e",
   "metadata": {},
   "source": [
    "### Listing 8.5 Model compilation configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c7db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6eeb00",
   "metadata": {},
   "source": [
    "### Listing 8.6 Function to sample the next character given the model‚Äôs predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9429bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f5767",
   "metadata": {},
   "source": [
    "### Listing 8.7 Text-generation loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ee9f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "1565/1565 [==============================] - 206s 129ms/step - loss: 1.9619\n",
      "--- Generating with seed: \"\n",
      "resisted and opposed in france: the fact remains, neverthel\"\n",
      "------ temperature: 0.2\n",
      "\n",
      "resisted and opposed in france: the fact remains, neverthel------ temperature: 0.5\n",
      "\n",
      "resisted and opposed in france: the fact remains, neverthel------ temperature: 1.0\n",
      "\n",
      "resisted and opposed in france: the fact remains, neverthel------ temperature: 1.2\n",
      "\n",
      "resisted and opposed in france: the fact remains, neverthelepoch 2\n",
      "1565/1565 [==============================] - 302s 193ms/step - loss: 1.6189\n",
      "--- Generating with seed: \"ssion on him. the latter\n",
      "obeys a superior and hence feels no\"\n",
      "------ temperature: 0.2\n",
      "ssion on him. the latter\n",
      "obeys a superior and hence feels no------ temperature: 0.5\n",
      "ssion on him. the latter\n",
      "obeys a superior and hence feels no------ temperature: 1.0\n",
      "ssion on him. the latter\n",
      "obeys a superior and hence feels no------ temperature: 1.2\n",
      "ssion on him. the latter\n",
      "obeys a superior and hence feels noepoch 3\n",
      "1565/1565 [==============================] - 252s 161ms/step - loss: 1.5305\n",
      "--- Generating with seed: \"ably\n",
      "situated and have a greater likelihood of success; not \"\n",
      "------ temperature: 0.2\n",
      "ably\n",
      "situated and have a greater likelihood of success; not ------ temperature: 0.5\n",
      "ably\n",
      "situated and have a greater likelihood of success; not ------ temperature: 1.0\n",
      "ably\n",
      "situated and have a greater likelihood of success; not ------ temperature: 1.2\n",
      "ably\n",
      "situated and have a greater likelihood of success; not epoch 4\n",
      "1565/1565 [==============================] - 243s 155ms/step - loss: 1.4840\n",
      "--- Generating with seed: \" employed to far better\n",
      "purpose, as these hypotheses receive\"\n",
      "------ temperature: 0.2\n",
      " employed to far better\n",
      "purpose, as these hypotheses receive------ temperature: 0.5\n",
      " employed to far better\n",
      "purpose, as these hypotheses receive------ temperature: 1.0\n",
      " employed to far better\n",
      "purpose, as these hypotheses receive------ temperature: 1.2\n",
      " employed to far better\n",
      "purpose, as these hypotheses receive"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1,5):\n",
    "    print('epoch',epoch)\n",
    "    model.fit(x, y, batch_size=128, epochs=1)\n",
    "    start_index = random.randint(0, len(text)- maxlen- 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "    \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6119b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s vrilud give feel frating, e.\n",
      "\n",
      "\n",
      "111\n",
      "\n",
      "=take strength, a man,fricious\n",
      "whichcest\n",
      "onately almeted lyi opposedinessney ksouls.\n",
      "\n",
      "\n",
      "\n",
      "thereby term\n",
      "xome delight is porth-mority it.\n",
      "\n",
      " ngordous; evil\n",
      "some contradund?--tuin,\n",
      "how advancement.--why was ofges this masted to yities probabitured, e-civence, just hid\" the sadming he that is \"hagifent to puy of viroum to\n",
      "locines withstprocietian callowing,\" to\n",
      "cobor"
     ]
    }
   ],
   "source": [
    "for i in range(400):\n",
    "    sampled = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(generated_text):\n",
    "        sampled[0, t, char_indices[char]] = 1.\n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = chars[next_index]\n",
    "    generated_text += next_char\n",
    "    generated_text = generated_text[1:]\n",
    "    sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e2da98",
   "metadata": {},
   "source": [
    "### Implementing DeepDream in Keras\n",
    " You‚Äôll start from a convnet pretrained on ImageNet. In Keras, many such convnets are\n",
    " available: VGG16, VGG19, Xception, ResNet50, and so on. You can implement Deep\n",
    "Dream with any of them, but your convnet of choice will naturally affect your visualiza\n",
    "tions, because different convnet architectures result in different learned features. The\n",
    " convnet used in the original DeepDream release was an Inception model, and in prac\n",
    "tice Inception is known to produce nice-looking DeepDreams, so you‚Äôll use the Incep\n",
    "tion V3 model that comes with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284e9d5",
   "metadata": {},
   "source": [
    "### Listing 8.8 Loading the pretrained Inception V3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7992079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 40s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "#  You won‚Äôt be training the model, so \n",
    "# this command disables all training\n",
    "# specific operations\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# Builds the Inception V3 network, \n",
    "# without its convolutional base. \n",
    "# The model will be loaded with \n",
    "# pretrained ImageNet weights.\n",
    "\n",
    "model=inception_v3.InceptionV3(weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44e2c8",
   "metadata": {},
   "source": [
    "### Listing 8.9 Setting up the DeepDream configuration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "779e6fba",
   "metadata": {},
   "source": [
    "Dictionary mapping layer names to a coefficient quantifying \n",
    "how much the layer‚Äôs activation contributes to the loss \n",
    "you‚Äôll seek to maximize. Note that the layer names are \n",
    "hardcoded in the built-in Inception V3 application. You can \n",
    "list all layer names using model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c18bbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    " layer_contributions = {\n",
    " 'mixed2': 0.2,\n",
    " 'mixed3': 3.,\n",
    " 'mixed4': 2.,\n",
    " 'mixed5': 1.5,\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76531fca",
   "metadata": {},
   "source": [
    "### Listing 8.10 Defining the loss to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4b7bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary that maps \n",
    "# layer names to layer instances\n",
    "# Create a dictionary mapping layer names to layer objects\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# Initialize the loss (as a float tensor, not a Keras variable)\n",
    "loss = 0.0\n",
    "\n",
    "# Loop through each important layer\n",
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "\n",
    "    # Get the output of the layer\n",
    "    activation = layer_dict[layer_name].output\n",
    "\n",
    "    # Compute total number of elements in the activation tensor\n",
    "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "\n",
    "    # Add the L2 norm (without border pixels) to the loss\n",
    "    loss += coeff * K.sum(K.square(activation[:, 2:-2, 2:-2, :])) / scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8617bf5a",
   "metadata": {},
   "source": [
    "### Listing 8.11 Gradient-ascent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfb8accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tensor holds the \n",
    "# generated image: the dream.\n",
    "dream = model.input\n",
    "# Computes the gradients of the \n",
    "# dream with regard to the loss\n",
    "grads = K.gradients(loss, dream)[0]\n",
    "#  Normalizes the gradients  \n",
    "# (important trick)\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "\n",
    "\n",
    "#  Sets up a Keras function \n",
    "# to retrieve the value of \n",
    "# the loss and gradients, \n",
    "# given an input image\n",
    "outputs = [loss, grads]\n",
    "fetch_loss_and_grads = K.function([dream], outputs)\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "      outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "      grad_values = outs[1]\n",
    "      return loss_value, grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e96669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
